================================================================================
FILE: .gitignore
================================================================================
# Rust build artifacts
/target/
/docs/
/Cargo.lock

# Logs and temp files
*.log
*.tmp
*.bak

# Generated AI pack and test fixtures
ai-pack/
tests/tmp/
tmp/
*.manifest.json

# Editor/OS files
.DS_Store
Thumbs.db
.idea/
.vscode/
*.swp
*.swo

# Git / OS cruft
*~
*.orig

# Node / frontend extras
node_modules/
dist/
build/
coverage/
.env

# Python extras
__pycache__/
.venv/
venv/

# Repomix or AI output
*.xml
*.jsonl

# Ignore everything inside /target/release except binaries we care about
!/target/release/saccade.exe
!/target/release/gauntlet.exe

================================================================================
FILE: Cargo.toml
================================================================================
[package]
name = "warden"
version = "0.3.0"
edition = "2021"

[lib]
name = "warden_core"
path = "src/lib.rs"

[[bin]]
name = "warden"
path = "src/bin/warden.rs"

[[bin]]
name = "knit"
path = "src/bin/knit.rs"

[dependencies]
anyhow = "1.0"
thiserror = "1.0"
regex = "1.10"
walkdir = "2.5"
clap = { version = "4.5", features = ["derive"] }
ignore = "0.4"
colored = "2.1"
rayon = "1.10"
once_cell = "1.19"
serde = { version = "1.0", features = ["derive"] }
toml = "0.8"

# THE BRAINS
tiktoken-rs = "0.5"

# Structural Parsing
tree-sitter = "0.20"
tree-sitter-rust = "0.20"
tree-sitter-python = "0.20"
tree-sitter-typescript = "0.20"
tree-sitter-javascript = "0.20"


================================================================================
FILE: LICENSE
================================================================================
MIT License

Copyright (c) 2025 Spencer Nunamaker

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.


================================================================================
FILE: README.md
================================================================================
# üõ°Ô∏è Warden Protocol

**Architecture Governance for the AI Era.**

> *"We do not ask the AI to write good code. We enforce good code via mechanical constraints."*

Warden is a local toolchain designed to enforce **Code With Intent (POT)**. It solves the "Context Drift" and "Hallucination" problems common in AI coding by enforcing strict structural discipline (Atomicity, Naming, Safety) before code is committed.

**v0.3.0 Update:** Warden now uses **Tree-sitter** for structural AST analysis and **Tiktoken** for LLM-native limits. Logic checks are now semantic, not just text-based.

## The Ecosystem

This repository contains two binaries that share a single logic core:

1.  **`warden` (The Enforcer):** An AST-based linter that rejects bloat (tokens), complexity (naming), and unsafe code (scope analysis).
2.  **`knit` (The Messenger):** A smart context-packer that serializes your repository for AI consumption, reporting exactly how many tokens you are feeding the model.

---

## 1. The Warden (Linter)

Warden checks if your code is **maintainable**. It enforces the "3 Laws" of this architecture:

### The 3 Laws
1.  **The Law of Atomicity (Anti-Bloat)**
    *   **Rule:** No file may exceed **2000 Tokens** (approx. 200-250 lines of dense code).
    *   **Goal:** Forces modularity based on **Attention Span**.
2.  **The Law of Bluntness (Naming)**
    *   **Rule:** Function names must be **‚â§ 3 words** (e.g., `fetchUser` ‚úÖ, `fetchUserAndSaveToDb` ‚ùå).
    *   **Goal:** Enforces Single Responsibility Principle (SRP).
3.  **The Law of Paranoia (Safety)**
    *   **Rule:** Logic bodies must contain explicit error handling (`Result`, `try/catch`, `match`, `unwrap_or`, `?`).
    *   **Goal:** Prevents "Silent Failures." Warden uses **Tree-sitter** to verify that safety exists structurally within the AST.

### Usage
```bash
# Run inside any Git repo
warden
```

**Bypass:** To intentionally skip a file, add `// warden:ignore` to the top.

---

## 2. Knit (Context Packer)

Knit stitches your "Atomic" files into a single stream for LLMs.

### Usage
```bash
# Standard Text Output
knit

# XML Output (Optimized for Claude 3.5 / Newer Models)
knit --format xml
```

---

## ‚öôÔ∏è Configuration

Warden works out-of-the-box, but you can customize the "3 Laws" via `warden.toml` in your project root.

```toml
# warden.toml
[rules]
max_file_tokens = 2500      # Default: 2000
max_function_words = 4      # Default: 3
ignore_naming_on = ["tests"] # paths containing this string skip naming checks
```

To ignore specific files/folders, create a `.wardenignore` file:
```text
# .wardenignore
legacy_code/
experiments/
```

---

## ü§ñ The AI System Prompt

To make the AI obey Warden, paste this into your System Prompt / Custom Instructions:

```text
ROLE: High-Integrity Systems Architect.
CONTEXT: You are coding inside a strict "Code With Intent" environment enforced by a binary linter called Warden.

THE 3 LAWS (Non-Negotiable):
1. LAW OF ATOMICITY (Token Limits):
   - Files MUST be < 2000 Tokens (~200 lines).
   - If a file grows too large, split it immediately.

2. LAW OF PARANOIA (Scope Safety):
   - Logic Blocks MUST contain explicit error handling (Result, try/catch, Option, ?) INSIDE the function body.
   - No unwrap() allowed.

3. LAW OF BLUNTNESS (Naming):
   - Function names Max 3 words (e.g., `fetchData` is good; `fetchDataAndProcess` is bad).

OPERATIONAL PROTOCOL:
1. Scan: Read the provided context.
2. Generate: Output WHOLE FILES with the filename in a header.
3. Verify: Ask yourself: "Will Warden reject this?" before printing.
```

---

**License:** MIT
```


================================================================================
FILE: src/analysis.rs
================================================================================
use crate::config::RuleConfig;
use tree_sitter::{Node, Parser, Query, QueryCursor};

pub struct Analyzer {
    rust_naming: Query,
    rust_safety: Query,
    rust_banned: Query,
    js_naming: Query,
    js_safety: Query,
    py_naming: Query,
    py_safety: Query,
}

impl Default for Analyzer {
    fn default() -> Self {
        Self::new()
    }
}

impl Analyzer {
    /// Compiles Tree-sitter queries.
    ///
    /// # Panics
    ///
    /// Panics if the internal hardcoded queries are invalid. This implies a development error.
    #[must_use]
    pub fn new() -> Self {
        Self {
            rust_naming: Query::new(tree_sitter_rust::language(), "(function_item name: (identifier) @name)").unwrap(),
            // Safety: Includes match, if let, while let, ?, and specific safety methods
            rust_safety: Query::new(tree_sitter_rust::language(), r#"
                (match_expression) @safe
                (if_expression condition: (let_condition)) @safe
                (while_expression condition: (let_condition)) @safe
                (try_expression) @safe
                (call_expression function: (field_expression field: (field_identifier) @m (#match? @m "^(expect|unwrap_or|unwrap_or_else|unwrap_or_default|ok|err|map_err|any|all|find|is_some|is_none|is_ok|is_err)$"))) @safe
                (call_expression function: (identifier) @f (#match? @f "^(Ok|Err)$")) @safe
            "#).unwrap(),
            // Banned: Explicitly hunt for unwrap() calls
            rust_banned: Query::new(tree_sitter_rust::language(), r#"
                (call_expression function: (field_expression field: (field_identifier) @m (#eq? @m "unwrap"))) @banned
            "#).unwrap(),

            js_naming: Query::new(tree_sitter_javascript::language(), r"
                (function_declaration name: (identifier) @name)
                (method_definition name: (property_identifier) @name)
                (variable_declarator name: (identifier) @name value: [(arrow_function) (function_expression)])
            ").unwrap(),
            js_safety: Query::new(tree_sitter_javascript::language(), r#"
                (try_statement) @safe
                (call_expression function: (member_expression property: (property_identifier) @m (#eq? @m "catch"))) @safe
            "#).unwrap(),

            py_naming: Query::new(tree_sitter_python::language(), "(function_definition name: (identifier) @name)").unwrap(),
            // Python Safety: Specific checks for 'try', 'not ...', and comparisons against 'None'
            py_safety: Query::new(tree_sitter_python::language(), r"
                (try_statement) @safe
                (if_statement condition: (unary_operator operator: (not_operator))) @safe
                (if_statement condition: (comparison_operator (_) (none))) @safe
            ").unwrap(),
        }
    }

    /// Analyzes the content for violations.
    ///
    /// # Panics
    ///
    /// Panics if the Tree-sitter parser fails to initialize the language.
    #[must_use]
    pub fn analyze(
        &self,
        lang: &str,
        filename: &str,
        content: &str,
        config: &RuleConfig,
    ) -> Vec<Violation> {
        let (language, naming_q, safety_q, banned_q) = match lang {
            "rs" => (
                tree_sitter_rust::language(),
                &self.rust_naming,
                &self.rust_safety,
                Some(&self.rust_banned),
            ),
            "js" | "jsx" | "ts" | "tsx" => (
                tree_sitter_typescript::language_typescript(),
                &self.js_naming,
                &self.js_safety,
                None,
            ),
            "py" => (
                tree_sitter_python::language(),
                &self.py_naming,
                &self.py_safety,
                None,
            ),
            _ => return vec![],
        };

        let mut parser = Parser::new();
        parser
            .set_language(language)
            .expect("Failed to load language");
        let tree = parser.parse(content, None).expect("Failed to parse");
        let root = tree.root_node();

        let mut violations = Vec::new();
        Self::check_naming(root, content, filename, naming_q, config, &mut violations);
        Self::check_safety(root, content, safety_q, &mut violations);

        if let Some(bq) = banned_q {
            Self::check_banned(root, content, bq, &mut violations);
        }

        violations
    }

    fn check_naming(
        root: Node,
        source: &str,
        filename: &str,
        query: &Query,
        config: &RuleConfig,
        out: &mut Vec<Violation>,
    ) {
        let mut cursor = QueryCursor::new();
        for m in cursor.matches(query, root, source.as_bytes()) {
            let node = m.captures[0].node;
            let name = node.utf8_text(source.as_bytes()).unwrap_or("?");

            let word_count = if name.contains('_') {
                name.split('_').count()
            } else {
                name.chars().filter(|c| c.is_uppercase()).count() + 1
            };

            let should_ignore = config.ignore_naming_on.iter().any(|p| filename.contains(p));

            if word_count > config.max_function_words && !should_ignore {
                out.push(Violation {
                    row: node.start_position().row,
                    message: format!(
                        "Function '{name}' has {word_count} words (Max: {})",
                        config.max_function_words
                    ),
                    law: "LAW OF BLUNTNESS",
                });
            }
        }
    }

    fn check_safety(root: Node, source: &str, safety_query: &Query, out: &mut Vec<Violation>) {
        let mut cursor = root.walk();
        loop {
            let node = cursor.node();
            let kind = node.kind();

            if (kind.contains("function") || kind.contains("method"))
                && !Self::is_lifecycle(node, source)
            {
                let mut func_cursor = QueryCursor::new();
                if func_cursor
                    .matches(safety_query, node, source.as_bytes())
                    .next()
                    .is_none()
                {
                    let rows = node.end_position().row - node.start_position().row;
                    if rows > 5 {
                        out.push(Violation {
                            row: node.start_position().row,
                            message:
                                "Logic block lacks structural safety (try/catch, match, Result, ?)."
                                    .into(),
                            law: "LAW OF PARANOIA",
                        });
                    }
                }
            }

            if !cursor.goto_first_child() {
                while !cursor.goto_next_sibling() {
                    if !cursor.goto_parent() {
                        return;
                    }
                }
            }
        }
    }

    fn check_banned(root: Node, source: &str, banned_query: &Query, out: &mut Vec<Violation>) {
        let mut cursor = QueryCursor::new();
        for m in cursor.matches(banned_query, root, source.as_bytes()) {
            let node = m.captures[0].node;
            out.push(Violation {
                row: node.start_position().row,
                message: "Explicit 'unwrap()' call detected. Use 'expect', 'unwrap_or', or '?'."
                    .into(),
                law: "LAW OF PARANOIA",
            });
        }
    }

    fn is_lifecycle(node: Node, source: &str) -> bool {
        if let Some(name_node) = node.child_by_field_name("name") {
            let name = name_node.utf8_text(source.as_bytes()).unwrap_or("");
            return matches!(
                name,
                "new" | "default" | "init" | "__init__" | "constructor" | "render" | "main"
            );
        }
        false
    }
}

pub struct Violation {
    pub row: usize,
    pub message: String,
    pub law: &'static str,
}


================================================================================
FILE: src/bin/knit.rs
================================================================================
use anyhow::Result;
use clap::{Parser, ValueEnum};
use colored::Colorize;
use std::fmt::Write;
use std::fs;
use std::path::Path;

use warden_core::config::{Config, GitMode};
use warden_core::enumerate::FileEnumerator;
use warden_core::filter::FileFilter;
use warden_core::heuristics::HeuristicFilter;
use warden_core::tokens::Tokenizer;

#[derive(Debug, Clone, ValueEnum)]
enum OutputFormat {
    Text,
    Xml,
}

#[derive(Parser)]
#[command(name = "knit")]
#[command(about = "Stitches atomic files into a single context file.")]
#[allow(clippy::struct_excessive_bools)]
struct Cli {
    #[arg(long, short)]
    stdout: bool,
    #[arg(long, short)]
    verbose: bool,
    #[arg(long)]
    git_only: bool,
    #[arg(long)]
    no_git: bool,
    #[arg(long)]
    code_only: bool,
    /// Output format (Text for standard, Xml for Claude/LLMs)
    #[arg(long, value_enum, default_value_t = OutputFormat::Text)]
    format: OutputFormat,
}

fn main() -> Result<()> {
    let cli = Cli::parse();

    let mut config = Config::new();
    config.verbose = cli.verbose;
    config.code_only = cli.code_only;

    if cli.git_only {
        config.git_mode = GitMode::Yes;
    } else if cli.no_git {
        config.git_mode = GitMode::No;
    }

    config.load_local_config();
    config.validate()?;

    if !cli.stdout {
        println!("üß∂ Knitting repository...");
    }

    let enumerator = FileEnumerator::new(config.clone());
    let raw_files = enumerator.enumerate()?;

    let heuristic_filter = HeuristicFilter::new();
    let heuristics_files = heuristic_filter.filter(raw_files);

    let filter = FileFilter::new(config.clone())?;
    let target_files = filter.filter(heuristics_files);

    if cli.verbose {
        eprintln!("üì¶ Packing {} files...", target_files.len());
    }

    let mut full_context = String::with_capacity(100_000);

    match cli.format {
        OutputFormat::Text => pack_text(&target_files, &mut full_context),
        OutputFormat::Xml => pack_xml(&target_files, &mut full_context),
    }

    // Count tokens
    let token_count = Tokenizer::count(&full_context);

    if cli.stdout {
        print!("{full_context}");
        eprintln!(
            "\nüìä Context Size: {} tokens",
            token_count.to_string().yellow().bold()
        );
    } else {
        fs::write("context.txt", &full_context)?;
        println!("‚úÖ Generated 'context.txt'");
        println!(
            "üìä Context Size: {} tokens",
            token_count.to_string().yellow().bold()
        );
    }

    Ok(())
}

fn normalize_path(path: &Path) -> String {
    path.to_string_lossy().replace('\\', "/")
}

fn pack_text(files: &[std::path::PathBuf], out: &mut String) {
    for path in files {
        let path_str = normalize_path(path);
        writeln!(
            out,
            "================================================================================"
        )
        .unwrap();
        writeln!(out, "FILE: {path_str}").unwrap();
        writeln!(
            out,
            "================================================================================"
        )
        .unwrap();

        match fs::read_to_string(path) {
            Ok(content) => {
                out.push_str(&content);
            }
            Err(e) => {
                writeln!(out, "<ERROR READING FILE: {e}>").unwrap();
            }
        }
        out.push_str("\n\n");
    }
}

fn pack_xml(files: &[std::path::PathBuf], out: &mut String) {
    writeln!(out, "<documents>").unwrap();
    for path in files {
        let path_str = normalize_path(path);
        // Security: Open CDATA tag
        writeln!(out, "  <document path=\"{path_str}\"><![CDATA[").unwrap();

        match fs::read_to_string(path) {
            Ok(content) => {
                // Security: Sanitize CDATA closing tags in content to prevent injection
                let safe_content = content.replace("]]>", "]]]]><![CDATA[>");
                out.push_str(&safe_content);
            }
            Err(e) => {
                writeln!(out, "ERROR READING FILE: {e}").unwrap();
            }
        }
        // Security: Close CDATA tag
        writeln!(out, "]]></document>").unwrap();
    }
    writeln!(out, "</documents>").unwrap();
}


================================================================================
FILE: src/bin/warden.rs
================================================================================
use anyhow::Result;
use clap::Parser;
use colored::Colorize;
use std::fs;
use std::process;

use warden_core::config::{Config, GitMode};
use warden_core::detection::Detector;
use warden_core::enumerate::FileEnumerator;
use warden_core::filter::FileFilter;
use warden_core::heuristics::HeuristicFilter;
use warden_core::rules::RuleEngine;

const DEFAULT_TOML: &str = r#"# warden.toml
[rules]
# The Law of Atomicity: Keep files small.
max_file_tokens = 2000

# The Law of Bluntness: Keep names simple.
max_function_words = 3

# Exclusions for naming rules (e.g. tests often have long names)
ignore_naming_on = ["tests", "spec"]
"#;

#[derive(Parser)]
#[command(name = "warden")]
#[command(about = "Structural linter for Code With Intent")]
#[allow(clippy::struct_excessive_bools)]
struct Cli {
    #[arg(long, short)]
    verbose: bool,
    #[arg(long)]
    git_only: bool,
    #[arg(long)]
    no_git: bool,
    #[arg(long)]
    code_only: bool,

    /// Initialize a default warden.toml configuration file
    #[arg(long)]
    init: bool,
}

fn main() -> Result<()> {
    let cli = Cli::parse();

    // 0. Init Mode
    if cli.init {
        if std::path::Path::new("warden.toml").exists() {
            println!("{}", "‚ö†Ô∏è warden.toml already exists.".yellow());
            return Ok(());
        }
        fs::write("warden.toml", DEFAULT_TOML)?;
        println!("{}", "‚úÖ Created warden.toml".green());
        return Ok(());
    }

    // 1. Setup & Validation
    let mut config = Config::new();
    config.verbose = cli.verbose;
    config.code_only = cli.code_only;

    if cli.git_only {
        config.git_mode = GitMode::Yes;
    } else if cli.no_git {
        config.git_mode = GitMode::No;
    }

    // Load local configuration (warden.toml / .wardenignore)
    config.load_local_config();
    config.validate()?;

    let enumerator = FileEnumerator::new(config.clone());
    let raw_files = enumerator.enumerate()?;

    // Context: Detection
    let detector = Detector::new();
    if let Ok(systems) = detector.detect_build_systems(&raw_files) {
        if !systems.is_empty() && config.verbose {
            let sys_list: Vec<String> = systems.iter().map(ToString::to_string).collect();
            println!("üîé Detected Ecosystem: [{}]", sys_list.join(", ").cyan());
        }
    }

    let heuristic_filter = HeuristicFilter::new();
    let heuristics_files = heuristic_filter.filter(raw_files);

    let filter = FileFilter::new(config.clone())?;
    let target_files = filter.filter(heuristics_files);

    if target_files.is_empty() {
        println!("No files to scan.");
        return Ok(());
    }

    println!(
        "üëÆ Warden scanning {} files (AST + Token Analysis)...",
        target_files.len()
    );

    // Logic Engine with injected Config
    let engine = RuleEngine::new(config);
    let mut total_failures = 0;

    for path in target_files {
        if let Ok(passed) = engine.check_file(&path) {
            if !passed {
                total_failures += 1;
            }
        }
    }

    if total_failures > 0 {
        println!(
            "{}",
            format!("‚ùå Warden found {total_failures} violations.")
                .red()
                .bold()
        );
        process::exit(1);
    } else {
        println!(
            "{}",
            "‚úÖ All Clear. Code structure is clean.".green().bold()
        );
        process::exit(0);
    }
}


================================================================================
FILE: src/config.rs
================================================================================
use crate::error::Result;
use regex::Regex;
use serde::Deserialize;
use std::fs;
use std::path::Path;

// --- CONFIG STRUCTURES ---

#[derive(Debug, Clone, Deserialize)]
pub struct RuleConfig {
    #[serde(default = "default_max_tokens")]
    pub max_file_tokens: usize,
    #[serde(default = "default_max_words")]
    pub max_function_words: usize,
    #[serde(default)]
    pub ignore_naming_on: Vec<String>,
}

impl Default for RuleConfig {
    fn default() -> Self {
        Self {
            max_file_tokens: default_max_tokens(),
            max_function_words: default_max_words(),
            ignore_naming_on: Vec::new(),
        }
    }
}

const fn default_max_tokens() -> usize {
    2000
}
const fn default_max_words() -> usize {
    3
}

#[derive(Debug, Clone, Deserialize, Default)]
pub struct WardenToml {
    #[serde(default)]
    pub rules: RuleConfig,
}

// --- MAIN CONFIG ---

#[derive(Debug, Clone)]
pub enum GitMode {
    Auto,
    Yes,
    No,
}

#[derive(Debug, Clone)]
pub struct Config {
    pub git_mode: GitMode,
    pub include_patterns: Vec<Regex>,
    pub exclude_patterns: Vec<Regex>,
    pub code_only: bool,
    pub verbose: bool,
    pub rules: RuleConfig,
}

impl Default for Config {
    fn default() -> Self {
        Self::new()
    }
}

impl Config {
    #[must_use]
    pub fn new() -> Self {
        Self {
            git_mode: GitMode::Auto,
            include_patterns: Vec::new(),
            exclude_patterns: Vec::new(),
            code_only: false,
            verbose: false,
            rules: RuleConfig::default(),
        }
    }

    /// Validates the configuration.
    ///
    /// # Errors
    ///
    /// Currently always returns `Ok`. Reserved for future validation logic
    /// that might return an error if configurations are invalid.
    pub fn validate(&self) -> Result<()> {
        Ok(())
    }

    pub fn load_local_config(&mut self) {
        self.load_ignore_file();
        self.load_toml_config();
    }

    fn load_ignore_file(&mut self) {
        let ignore_path = Path::new(".wardenignore");
        if let Ok(content) = fs::read_to_string(ignore_path) {
            for line in content.lines() {
                let trimmed = line.trim();
                if trimmed.is_empty() || trimmed.starts_with('#') {
                    continue;
                }
                if let Ok(re) = Regex::new(trimmed) {
                    self.exclude_patterns.push(re);
                }
            }
        }
    }

    fn load_toml_config(&mut self) {
        let toml_path = Path::new("warden.toml");
        if let Ok(content) = fs::read_to_string(toml_path) {
            if let Ok(parsed) = toml::from_str::<WardenToml>(&content) {
                self.rules = parsed.rules;
                if self.verbose {
                    println!("üîß Loaded warden.toml configuration");
                }
            } else if self.verbose {
                eprintln!("‚ö†Ô∏è Failed to parse warden.toml");
            }
        }
    }
}

// Constants for automatic pruning (Smart Defaults)
pub const PRUNE_DIRS: &[&str] = &[
    ".git",
    "node_modules",
    "target",
    "dist",
    "build",
    "out",
    "gen",
    ".venv",
    "venv",
    ".tox",
    "__pycache__",
    "coverage",
    "vendor",
];

pub const BIN_EXT_PATTERN: &str =
    r"(?i)\.(png|jpg|gif|svg|ico|webp|woff2?|ttf|pdf|mp4|zip|gz|tar|exe|dll|so|dylib|class|pyc)$";
pub const SECRET_PATTERN: &str =
    r"(?i)(^\.?env(\..*)?$|/\.?env(\..*)?$|(^|/)(id_rsa|id_ed25519|.*\.(pem|p12|key|pfx))$)";
pub const CODE_EXT_PATTERN: &str = r"(?i)\.(rs|go|py|js|jsx|ts|tsx|java|c|cpp|h|hpp|cs|php|rb|sh|sql|html|css|scss|json|toml|yaml|md)$";
pub const CODE_BARE_PATTERN: &str = r"(?i)(Makefile|Dockerfile|CMakeLists\.txt)$";


================================================================================
FILE: src/detection.rs
================================================================================
use crate::error::Result;
use std::collections::HashSet;
use std::fmt;
use std::path::Path;

#[derive(Debug, PartialEq, Eq, Hash, Clone, Copy)]
pub enum BuildSystemType {
    Rust,
    Node,
    Python,
    Go,
    CMake,
    Conan,
}

impl fmt::Display for BuildSystemType {
    fn fmt(&self, f: &mut fmt::Formatter<'_>) -> fmt::Result {
        write!(f, "{self:?}")
    }
}

pub struct Detector;

impl Detector {
    #[must_use]
    pub fn new() -> Self {
        Self
    }

    /// Detects build systems in the file list.
    ///
    /// # Errors
    ///
    /// Returns error if underlying detection logic fails.
    pub fn detect_build_systems(
        &self,
        files: &[std::path::PathBuf],
    ) -> Result<Vec<BuildSystemType>> {
        let mut detected = HashSet::new();

        for file in files {
            if Self::is_cargo(file) {
                detected.insert(BuildSystemType::Rust);
            }
            if Self::is_npm(file) {
                detected.insert(BuildSystemType::Node);
            }
            if Self::is_python(file) {
                detected.insert(BuildSystemType::Python);
            }
            if Self::is_go(file) {
                detected.insert(BuildSystemType::Go);
            }
            if Self::is_cmake(file) {
                detected.insert(BuildSystemType::CMake);
            }
            if Self::is_conan(file) {
                detected.insert(BuildSystemType::Conan);
            }
        }

        Ok(detected.into_iter().collect())
    }

    fn is_cargo(path: &Path) -> bool {
        path.ends_with("Cargo.toml")
    }
    fn is_npm(path: &Path) -> bool {
        path.ends_with("package.json")
    }
    fn is_python(path: &Path) -> bool {
        matches!(
            path.file_name().and_then(|n| n.to_str()),
            Some("requirements.txt" | "pyproject.toml" | "Pipfile")
        )
    }
    fn is_go(path: &Path) -> bool {
        path.ends_with("go.mod")
    }
    fn is_cmake(path: &Path) -> bool {
        let s = path.to_string_lossy();
        s.contains("CMakeLists.txt") || s.ends_with(".cmake")
    }
    fn is_conan(path: &Path) -> bool {
        matches!(
            path.file_name().and_then(|n| n.to_str()),
            Some("conanfile.txt" | "conanfile.py")
        )
    }
}

impl Default for Detector {
    fn default() -> Self {
        Self::new()
    }
}


================================================================================
FILE: src/enumerate.rs
================================================================================
// warden:ignore
use crate::config::{Config, PRUNE_DIRS};
use crate::error::{Result, WardenError};
use std::path::PathBuf;
use std::process::Command;
use walkdir::WalkDir;

pub struct FileEnumerator {
    config: Config,
}

impl FileEnumerator {
    #[must_use]
    pub fn new(config: Config) -> Self {
        Self { config }
    }

    /// Enumerates files based on configuration.
    ///
    /// # Errors
    ///
    /// Returns error if `git` fails in Git mode.
    pub fn enumerate(&self) -> Result<Vec<PathBuf>> {
        use crate::config::GitMode;

        match self.config.git_mode {
            GitMode::Yes => {
                if !Self::in_git_repo() {
                    return Err(WardenError::NotInGitRepo);
                }
                // Fixed: Self::filter_paths
                Ok(Self::filter_paths(Self::git_ls_files()?))
            }
            GitMode::No => Ok(self.walk_all_files()),
            GitMode::Auto => {
                if Self::in_git_repo() {
                    if let Ok(files) = Self::git_ls_files() {
                        // Fixed: Self::filter_paths
                        return Ok(Self::filter_paths(files));
                    }
                }
                Ok(self.walk_all_files())
            }
        }
    }

    // Fixed: Removed &self
    fn filter_paths(paths: Vec<PathBuf>) -> Vec<PathBuf> {
        paths
            .into_iter()
            .filter(|p| {
                for part in p.components() {
                    if let Some(s) = part.as_os_str().to_str() {
                        if PRUNE_DIRS.contains(&s) {
                            return false;
                        }
                    }
                }
                true
            })
            .collect()
    }

    fn in_git_repo() -> bool {
        let out = Command::new("git")
            .arg("rev-parse")
            .arg("--is-inside-work-tree")
            .output();

        matches!(out, Ok(o) if o.status.success())
    }

    fn git_ls_files() -> Result<Vec<PathBuf>> {
        let out = Command::new("git")
            .arg("ls-files")
            .arg("-z")
            .arg("--exclude-standard")
            .output()?;

        if !out.status.success() {
            return Err(WardenError::Other(format!(
                "git ls-files failed: exit {}",
                out.status
            )));
        }

        let mut paths = Vec::new();
        for chunk in out.stdout.split(|b| *b == 0) {
            if chunk.is_empty() {
                continue;
            }
            let s = String::from_utf8_lossy(chunk);
            paths.push(PathBuf::from(s.as_ref()));
        }
        Ok(paths)
    }

    fn walk_all_files(&self) -> Vec<PathBuf> {
        let mut paths = Vec::new();
        let mut errors = Vec::new();

        let walker = WalkDir::new(".").follow_links(false).into_iter();

        for item in walker.filter_entry(|e| {
            let name = e.file_name().to_string_lossy();
            // WalkDir filtering allows us to skip descending into "node_modules" entirely
            !PRUNE_DIRS.iter().any(|p| name == *p)
        }) {
            let entry = match item {
                Ok(e) => e,
                Err(e) => {
                    errors.push(format!("walkdir: {e}"));
                    continue;
                }
            };

            if entry.file_type().is_file() {
                let p = entry.path().strip_prefix(".").unwrap_or(entry.path());
                paths.push(p.to_path_buf());
            }
        }

        if !errors.is_empty() && self.config.verbose {
            eprintln!(
                "WARN: Encountered {} errors during file walk:",
                errors.len()
            );
            for (i, err) in errors.iter().take(5).enumerate() {
                eprintln!("  {}. {}", i + 1, err);
            }
        }

        paths
    }
}


================================================================================
FILE: src/error.rs
================================================================================
// src/error.rs
use std::path::PathBuf;
use thiserror::Error;

#[derive(Debug, Error)]
pub enum WardenError {
    #[error("I/O error: {source} (path: {path})")]
    Io {
        source: std::io::Error,
        path: PathBuf,
    },

    #[error("Not inside a Git repository")]
    NotInGitRepo,

    #[error("Regex error: {0}")]
    Regex(#[from] regex::Error),

    #[error("Generic error: {0}")]
    Other(String),
}

pub type Result<T> = std::result::Result<T, WardenError>;

// Allow `?` on std::io::Error by converting to WardenError::Io with unknown path.
impl From<std::io::Error> for WardenError {
    fn from(source: std::io::Error) -> Self {
        WardenError::Io {
            source,
            path: PathBuf::from("<unknown>"),
        }
    }
}

// Gracefully convert WalkDir errors
impl From<walkdir::Error> for WardenError {
    fn from(e: walkdir::Error) -> Self {
        WardenError::Other(e.to_string())
    }
}


================================================================================
FILE: src/filter.rs
================================================================================
use crate::config::{Config, BIN_EXT_PATTERN, CODE_BARE_PATTERN, CODE_EXT_PATTERN, SECRET_PATTERN};
use crate::error::Result;
use regex::Regex;
use std::path::Path;

pub struct FileFilter {
    config: Config,
    bin_ext_re: Regex,
    secret_re: Regex,
    code_ext_re: Option<Regex>,
    code_bare_re: Option<Regex>,
}

impl FileFilter {
    /// Creates a new file filter.
    ///
    /// # Errors
    ///
    /// Returns an error if any of the regex patterns (binary extensions, secrets, or code patterns) fail to compile.
    pub fn new(config: Config) -> Result<Self> {
        let bin_ext_re = Regex::new(BIN_EXT_PATTERN)?;
        let secret_re = Regex::new(SECRET_PATTERN)?;

        let (code_ext_re, code_bare_re) = if config.code_only {
            (
                Some(Regex::new(CODE_EXT_PATTERN)?),
                Some(Regex::new(CODE_BARE_PATTERN)?),
            )
        } else {
            (None, None)
        };

        Ok(Self {
            config,
            bin_ext_re,
            secret_re,
            code_ext_re,
            code_bare_re,
        })
    }

    #[must_use]
    pub fn filter(&self, files: Vec<std::path::PathBuf>) -> Vec<std::path::PathBuf> {
        files.into_iter().filter(|p| self.should_keep(p)).collect()
    }

    fn should_keep(&self, path: &Path) -> bool {
        let s = path.to_string_lossy().replace('\\', "/");

        // Structural Safety: Explicit truth table matching
        match (self.is_secret(&s), self.is_binary(&s), self.is_excluded(&s)) {
            (true, _, _) | (_, true, _) | (_, _, true) => return false,
            _ => {}
        }

        match (self.is_included(&s), self.config.code_only) {
            (false, _) => false,
            (_, true) if !self.is_code(&s) => false,
            _ => true,
        }
    }

    fn is_secret(&self, path: &str) -> bool {
        self.secret_re.is_match(path)
    }

    fn is_binary(&self, path: &str) -> bool {
        self.bin_ext_re.is_match(path)
    }

    fn is_excluded(&self, path: &str) -> bool {
        self.config
            .exclude_patterns
            .iter()
            .any(|p| p.is_match(path))
    }

    fn is_included(&self, path: &str) -> bool {
        self.config.include_patterns.is_empty()
            || self
                .config
                .include_patterns
                .iter()
                .any(|p| p.is_match(path))
    }

    fn is_code(&self, path: &str) -> bool {
        // Explicit structural safety for mixed option/regex logic
        match (&self.code_ext_re, &self.code_bare_re) {
            (Some(ext), Some(bare)) => ext.is_match(path) || bare.is_match(path),
            _ => true,
        }
    }
}


================================================================================
FILE: src/heuristics.rs
================================================================================
// warden:ignore
use crate::config::{CODE_BARE_PATTERN, CODE_EXT_PATTERN};
use regex::Regex;
use std::collections::HashMap;
use std::fs;
use std::path::Path;
use std::sync::LazyLock;

// --- Configuration Constants for Heuristics ---
const MIN_TEXT_ENTROPY: f64 = 3.5;
const MAX_TEXT_ENTROPY: f64 = 5.5;

const BUILD_SYSTEM_PAMPS: &[&str] = &[
    "find_package",
    "add_executable",
    "target_link_libraries",
    "cmake_minimum_required",
    "project(",
    "add-apt-repository",
    "conanfile.py",
    "dependency",
    "require",
    "include",
    "import",
    "version",
    "dependencies",
];

// Pre-compiled regexes for known code files
static CODE_EXT_RE: LazyLock<Regex> = LazyLock::new(|| Regex::new(CODE_EXT_PATTERN).unwrap());
static CODE_BARE_RE: LazyLock<Regex> = LazyLock::new(|| Regex::new(CODE_BARE_PATTERN).unwrap());

pub struct HeuristicFilter;

impl HeuristicFilter {
    #[must_use]
    pub fn new() -> Self {
        Self
    }

    #[must_use]
    pub fn filter(&self, files: Vec<std::path::PathBuf>) -> Vec<std::path::PathBuf> {
        files
            .into_iter()
            .filter(|path| Self::should_keep(path))
            .collect()
    }

    fn should_keep(path: &Path) -> bool {
        let path_str = path.to_string_lossy();

        if CODE_EXT_RE.is_match(&path_str) || CODE_BARE_RE.is_match(&path_str) {
            return true;
        }

        if let Ok(entropy) = calculate_entropy(path) {
            if !(MIN_TEXT_ENTROPY..=MAX_TEXT_ENTROPY).contains(&entropy) {
                return false;
            }
        } else {
            return false;
        }

        if let Ok(content) = fs::read_to_string(path) {
            let lower_content = content.to_lowercase();
            for pamp in BUILD_SYSTEM_PAMPS {
                if lower_content.contains(pamp) {
                    return true;
                }
            }
        }

        true
    }
}

impl Default for HeuristicFilter {
    fn default() -> Self {
        Self::new()
    }
}

fn calculate_entropy(path: &Path) -> std::io::Result<f64> {
    let bytes = fs::read(path)?;
    if bytes.is_empty() {
        return Ok(0.0);
    }

    let mut freq_map = HashMap::new();
    for &byte in &bytes {
        *freq_map.entry(byte).or_insert(0) += 1;
    }

    // Suppress cast precision loss for 64-bit length; entropy approximation is fine.
    #[allow(clippy::cast_precision_loss)]
    let len = bytes.len() as f64;

    let entropy = freq_map.values().fold(0.0, |acc, &count| {
        let probability = f64::from(count) / len;
        acc - probability * probability.log2()
    });

    Ok(entropy)
}


================================================================================
FILE: src/lib.rs
================================================================================
pub mod analysis;
pub mod config;
pub mod detection;
pub mod enumerate;
pub mod error;
pub mod filter;
pub mod heuristics;
pub mod rules;
pub mod tokens;


================================================================================
FILE: src/rules.rs
================================================================================
use crate::analysis::Analyzer;
use crate::config::Config;
use crate::error::Result;
use crate::tokens::Tokenizer;
use colored::Colorize;
use std::fs;
use std::path::Path;
use std::sync::LazyLock;

// Thread-safe analyzer instance
static ANALYZER: LazyLock<Analyzer> = LazyLock::new(Analyzer::new);

pub struct RuleEngine {
    config: Config,
}

impl RuleEngine {
    #[must_use]
    pub fn new(config: Config) -> Self {
        Self { config }
    }

    /// Checks a file for violations. Returns `Ok(false)` if violations found.
    ///
    /// # Errors
    ///
    /// Returns an error if the file cannot be read (and is not skipped).
    /// Note: Most read errors are suppressed/ignored in the current logic, returning `Ok(true)`.
    pub fn check_file(&self, path: &Path) -> Result<bool> {
        let Ok(content) = fs::read_to_string(path) else {
            return Ok(true); // Skip unreadable files
        };

        if content.contains("// warden:ignore") || content.contains("# warden:ignore") {
            return Ok(true);
        }

        let filename = path.to_string_lossy();
        let mut passed = true;

        // 1. LAW OF ATOMICITY (Token Limit)
        let token_count = Tokenizer::count(&content);
        if token_count > self.config.rules.max_file_tokens {
            Self::print_violation(
                &filename,
                0,
                &format!(
                    "File size is {token_count} tokens (Limit: {})",
                    self.config.rules.max_file_tokens
                ),
                "LAW OF ATOMICITY",
                "Split this file into smaller modules.",
            );
            passed = false;
        }

        // 2. AST ANALYSIS (Paranoia + Bluntness)
        if let Some(ext) = path.extension().and_then(|s| s.to_str()) {
            // Pass filename for rule config matching (ignore_naming_on)
            let violations = ANALYZER.analyze(ext, &filename, &content, &self.config.rules);

            for v in violations {
                Self::print_violation(
                    &filename,
                    v.row,
                    &v.message,
                    v.law,
                    if v.law == "LAW OF BLUNTNESS" {
                        "Rename function."
                    } else {
                        "Add Error Handling."
                    },
                );
                passed = false;
            }
        }

        Ok(passed)
    }

    fn print_violation(filename: &str, row: usize, msg: &str, law: &str, help: &str) {
        let line_num = row + 1;
        println!("{}: {}", "error".red().bold(), msg.bold());
        println!("  {} {}:{}:1", "-->".blue(), filename, line_num);
        println!("   {}", "|".blue());
        println!("   {} {}: {}", "=".blue().bold(), law.white().bold(), help);
        println!();
    }
}


================================================================================
FILE: src/tokens.rs
================================================================================
use std::sync::LazyLock;
use tiktoken_rs::CoreBPE;

// We use cl100k_base (GPT-4/3.5 turbo encoding) as the standard
static BPE: LazyLock<CoreBPE> = LazyLock::new(|| tiktoken_rs::cl100k_base().unwrap());

pub struct Tokenizer;

impl Tokenizer {
    #[must_use]
    pub fn count(text: &str) -> usize {
        // EncodeOrdinary is faster as it ignores special tokens, which is fine for code
        BPE.encode_ordinary(text).len()
    }

    /// Returns true if the file exceeds the token limit
    #[must_use]
    pub fn exceeds_limit(text: &str, limit: usize) -> bool {
        Self::count(text) > limit
    }
}


